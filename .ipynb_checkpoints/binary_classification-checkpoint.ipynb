{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silas/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import timeit\n",
    "import csv\n",
    "import os\n",
    "import string\n",
    "import textblob\n",
    "import requests\n",
    "import io\n",
    "import nltk\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold, cross_val_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, roc_curve, auc\n",
    "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score, silhouette_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, recall_score, precision_score, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "# from keras.preprocessing import text, sequence\n",
    "# from keras import layers, models, optimizers\n",
    "\n",
    "from importlib import reload\n",
    "# reload(text_processing)\n",
    "\n",
    "\n",
    "# nlp = spacy.load('en') # loading the language model \n",
    "#data = pd.read_feather('data/preprocessed_data') # reading a pandas dataframe which is stored as a feather file\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "# sns.set_context('poster')\n",
    "# sns.set_color_codes()\n",
    "full_data_path = '/home/silas/final_project/Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "- load data from file\n",
    "- inspect data\n",
    "- select for testing\n",
    "- merge with climate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climate = pd.read_pickle(full_data_path + \"climate.pkl\").drop_duplicates(subset='processed_text')\n",
    "df_spam = pd.read_pickle(full_data_path + \"spam.pkl\")\n",
    "df_newsgroups = pd.read_pickle(full_data_path + \"newsgroups.pkl\")\n",
    "df_reuters = pd.read_pickle(full_data_path + \"reuters.pkl\")\n",
    "df_bbc = pd.read_pickle(full_data_path + \"bbc.pkl\")\n",
    "df_science = pd.read_pickle(full_data_path + \"science.pkl\").drop_duplicates(subset='processed_text')\n",
    "# add dataframe to dictionary of dataframes\n",
    "df_datasets={'climate':df_climate,'spam':df_spam,'newsgroups':df_newsgroups,'reuters':df_reuters,'bbc':df_bbc,'science':df_science}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_counts(df, verbose=True):\n",
    "    count={}\n",
    "    count['count']=[len(i) for i in df.processed_text] \n",
    "    if verbose:\n",
    "        print('Avg doc length:     ', sum(count)/df.shape[0]), print(\"\")\n",
    "        print('Min doc length: ', min(count))\n",
    "        print('Max doc length:  ', min(count))\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(df_datasets=df_datasets):\n",
    "    count=0\n",
    "    lst,lstn,lstd,lstz,lstzz,lstv,lstvu,lstc=[],[],[],[],[],[],[],['total_docs', 'min_doc_len', 'max_doc_len', 'mean', 'std']\n",
    "    for k,v in df_datasets.items():\n",
    "        vd = get_doc_counts(v,verbose=False)\n",
    "        vdc = vd.get('count')\n",
    "        lst.append((len(vdc), min(vdc),max(vdc),int(np.mean(vdc)),int(np.std(vdc))))\n",
    "        lstn.append(k)\n",
    "        lstd.append(len(v.processed_text)-len(v.processed_text.unique()))\n",
    "        for i in v.processed_text:\n",
    "            if len(i) == 0:\n",
    "                count+=1\n",
    "        lstz.append(count)\n",
    "        cv = CountVectorizer()\n",
    "        cv.fit(v.texts)\n",
    "        lstvu.append(len(cv.vocabulary_))\n",
    "        cvu = CountVectorizer()\n",
    "        cvu.fit(v.processed_text)\n",
    "        lstv.append(len(cvu.vocabulary_))\n",
    "        count=0\n",
    "    \n",
    "    df = pd.DataFrame(lst,columns=lstc)\n",
    "    df['df_name']=lstn\n",
    "    df['dups']=lstd\n",
    "    df['zero_vals']=lstz\n",
    "    df['vocab_proc']=lstv\n",
    "    df['vocab_raw']=lstvu\n",
    "    \n",
    "    df=df.set_index('df_name')\n",
    "    df=df[['dups','zero_vals','total_docs','vocab_raw','vocab_proc','min_doc_len','max_doc_len','std']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dups</th>\n",
       "      <th>zero_vals</th>\n",
       "      <th>total_docs</th>\n",
       "      <th>vocab_raw</th>\n",
       "      <th>vocab_proc</th>\n",
       "      <th>min_doc_len</th>\n",
       "      <th>max_doc_len</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>df_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>climate</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6126</td>\n",
       "      <td>22486</td>\n",
       "      <td>13018</td>\n",
       "      <td>1229</td>\n",
       "      <td>8715</td>\n",
       "      <td>656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4889</td>\n",
       "      <td>8633</td>\n",
       "      <td>5782</td>\n",
       "      <td>0</td>\n",
       "      <td>361</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newsgroups</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10711</td>\n",
       "      <td>98439</td>\n",
       "      <td>55269</td>\n",
       "      <td>56</td>\n",
       "      <td>40834</td>\n",
       "      <td>1723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reuters</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10530</td>\n",
       "      <td>30913</td>\n",
       "      <td>21761</td>\n",
       "      <td>0</td>\n",
       "      <td>5610</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbc</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2114</td>\n",
       "      <td>29418</td>\n",
       "      <td>21626</td>\n",
       "      <td>0</td>\n",
       "      <td>12948</td>\n",
       "      <td>731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5735</td>\n",
       "      <td>42109</td>\n",
       "      <td>29684</td>\n",
       "      <td>47</td>\n",
       "      <td>2649</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            dups  zero_vals  total_docs  vocab_raw  vocab_proc  min_doc_len  \\\n",
       "df_name                                                                       \n",
       "climate        0          0        6126      22486       13018         1229   \n",
       "spam           0          1        4889       8633        5782            0   \n",
       "newsgroups     0          0       10711      98439       55269           56   \n",
       "reuters        0          1       10530      30913       21761            0   \n",
       "bbc            0          1        2114      29418       21626            0   \n",
       "science        0          0        5735      42109       29684           47   \n",
       "\n",
       "            max_doc_len   std  \n",
       "df_name                        \n",
       "climate            8715   656  \n",
       "spam                361    27  \n",
       "newsgroups        40834  1723  \n",
       "reuters            5610   505  \n",
       "bbc               12948   731  \n",
       "science            2649   302  "
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "# slice docs in climate data to get meat\n",
    "# make mask based on min/max length of merged dataset \n",
    "result_dict={}\n",
    "for k,v in df_datasets.items():\n",
    "    result_dict[k] = get_doc_counts(v,verbose=False)\n",
    "fig, axs = plt.subplots(2, len(result_dict), figsize=(15, 5), sharey=True,sharex=False)\n",
    "fig.suptitle('Distribution of Document Lengths (chars)')\n",
    "for idx,kv in enumerate(result_dict.items()):\n",
    "    axs[0,idx].hist(kv[1].get('count'))\n",
    "    axs[1,idx].boxplot(kv[1].get('count'))\n",
    "    axs[1,idx].set_xlabel(kv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_reuters.loc[df_reuters.labels=='acq',df_reuters.columns]\n",
    "# plt.hist(encoder.fit_transform(df_reuters.labels),bins=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x144 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_dict={}\n",
    "for k,v in df_datasets.items():\n",
    "    result_dict[k] = get_label_counts(v)\n",
    "fig, axs = plt.subplots(1, len(result_dict), figsize=(15, 2), sharey=True,sharex=False)\n",
    "fig.suptitle('Number of Classes (chars)')\n",
    "for idx,kv in enumerate(result_dict.items()):\n",
    "    axs[idx].hist(kv[1])\n",
    "    axs[idx].set_xlabel(kv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge climate with other data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_df(target_df,other_df):\n",
    "    '''\n",
    "    Args: df_other: a pd.dataframe with columns: 'texts','labels','processed_text'\n",
    "    Returns: a df with climate and random sample of other data 50/50 split\n",
    "    Note: uncomment other labels\n",
    "    '''\n",
    "    \n",
    "    df = target_df\n",
    "    length = min(df.shape[0],other_df.shape[0])\n",
    "    other_s = other_df.sample(length)\n",
    "    other_s.labels = 0\n",
    "    df_s = df.sample(length)\n",
    "    other_s.append(df_s)\n",
    "    \n",
    "    return other_s.append(df_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dups</th>\n",
       "      <th>zero_vals</th>\n",
       "      <th>total_docs</th>\n",
       "      <th>vocab_raw</th>\n",
       "      <th>vocab_proc</th>\n",
       "      <th>min_doc_len</th>\n",
       "      <th>max_doc_len</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>df_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>merged</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>12252</td>\n",
       "      <td>38130</td>\n",
       "      <td>24284</td>\n",
       "      <td>0</td>\n",
       "      <td>7480</td>\n",
       "      <td>643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dups  zero_vals  total_docs  vocab_raw  vocab_proc  min_doc_len  \\\n",
       "df_name                                                                    \n",
       "merged      3          1       12252      38130       24284            0   \n",
       "\n",
       "         max_doc_len  std  \n",
       "df_name                    \n",
       "merged          7480  643  "
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# climate reuters merge\n",
    "# remove unique pre formating text from climate dataframe before merge\n",
    "df_climate['processed_text']=[i[515:-700].split(' ',2)[2].rsplit(' ',1)[0] for i in df_climate.processed_text]\n",
    "# run merge\n",
    "mgd = merge_df(df_climate,df_reuters)\n",
    "mgd['processed_text']=mgd.processed_text.str.replace('subject','') # newsgroups has 'subject' in each doc; remvoval for parity with climate change\n",
    "# save merged df\n",
    "mgd.to_pickle(full_data_path + \"reuters_merged.pkl\")\n",
    "df_set = {}\n",
    "df_set['merged']=mgd.reset_index(drop=True)\n",
    "print_stats(df_set).to_csv('reuters_merged_stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dups</th>\n",
       "      <th>zero_vals</th>\n",
       "      <th>total_docs</th>\n",
       "      <th>vocab_raw</th>\n",
       "      <th>vocab_proc</th>\n",
       "      <th>min_doc_len</th>\n",
       "      <th>max_doc_len</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>df_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>merged</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11470</td>\n",
       "      <td>54454</td>\n",
       "      <td>36303</td>\n",
       "      <td>2</td>\n",
       "      <td>7480</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dups  zero_vals  total_docs  vocab_raw  vocab_proc  min_doc_len  \\\n",
       "df_name                                                                    \n",
       "merged      3          0       11470      54454       36303            2   \n",
       "\n",
       "         max_doc_len  std  \n",
       "df_name                    \n",
       "merged          7480  514  "
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# climate science merge --> load \"science_merged.pkl\"\n",
    "# remove unique pre formating text from climate dataframe before merge\n",
    "df_climate['processed_text']=[i[515:-700].split(' ',2)[2].rsplit(' ',1)[0] for i in df_climate.processed_text]\n",
    "# run merge\n",
    "mgd = merge_df(df_climate,df_science)\n",
    "mgd['processed_text']=mgd.processed_text.str.replace('subject','') # newsgroups has 'subject' in each doc; remvoval for parity with climate change\n",
    "# save merged df\n",
    "mgd.to_pickle(full_data_path + \"science_merged.pkl\")\n",
    "df_set = {}\n",
    "df_set['merged']=mgd.reset_index(drop=True)\n",
    "print_stats(df_set).to_csv('science_merged_stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unhash to inspect label count\n",
    "# df.groupby('labels')['texts'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataframe into text and labels\n",
    "# df = df_merged\n",
    "def pre_split_data(df):\n",
    "    texts = df.texts\n",
    "    labels = df.labels\n",
    "    processed_texts = df.processed_text\n",
    "    return  texts,labels,processed_texts\n",
    "\n",
    "texts,labels,processed_texts= pre_split_data(mgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNPROCESSED: raw text, split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y_text_label, valid_y_text_label = train_test_split(texts, labels)\n",
    "\n",
    "# PROCESSED: preprocessed text, split the dataset into training and validation datasets \n",
    "train_xp, valid_xp, train_yp_text_label, valid_yp_text_label = train_test_split(processed_texts, labels)\n",
    "\n",
    "# label encode the target variable for raw and processed datasets\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y_text_label)\n",
    "valid_y = encoder.fit_transform(valid_y_text_label)\n",
    "train_yp = encoder.fit_transform(train_yp_text_label)\n",
    "valid_yp = encoder.fit_transform(valid_yp_text_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarize data labels\n",
    "- for binary classification problem --> takes one category (ex: label '1') and makes '1' all others '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array((pd.DataFrame({'t_label':train_y}).t_label == 1).astype('int'))\n",
    "valid_y = np.array((pd.DataFrame({'t_label':valid_y}).t_label == 1).astype('int'))\n",
    "train_yp = np.array((pd.DataFrame({'t_label':train_yp}).t_label == 1).astype('int'))\n",
    "valid_yp = np.array((pd.DataFrame({'t_label':valid_yp}).t_label == 1).astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of run times for models\n",
    "time_keeper={}\n",
    "\n",
    "# set df name for labeling later on\n",
    "df_name = 'science'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "- https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params take form of (analyzer, token pattern, stop_words)\n",
    "\n",
    "def run_vectorizer(train_x,valid_x, model, params):\n",
    "\n",
    "    # create a vectorizer object \n",
    "    if model == CountVectorizer:\n",
    "        vect = model(analyzer=params[0], token_pattern=params[1], stop_words=params[2], \n",
    "                     lowercase=params[3], ngram_range=params[4])\n",
    "    \n",
    "    else:\n",
    "        vect = model(model(analyzer=params[0], max_df=params[1], min_df=params[2], max_features=params[3], \n",
    "          stop_words=params[4], use_idf=params[5]), token_pattern=params[6], ngram_range=params[7])\n",
    "    \n",
    "    \n",
    "    vect.fit(texts)\n",
    "\n",
    "    # transform the training and validation data using count vectorizer objectea\n",
    "    xtrain_ =  vect.transform(train_x)\n",
    "    xvalid_ =  vect.transform(valid_x)\n",
    "    \n",
    "    return xtrain_, xvalid_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CountVectorizer(input=content, encoding=utf-8, decode_error=strict, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=(?u)\\x08\\\\w\\\\w+\\x08, ngram_range=(1, 1), analyzer=word, max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class numpy.int64>)[source]\\n'"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''CountVectorizer(input=content, encoding=utf-8, decode_error=strict, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=(?u)\\b\\w\\w+\\b, ngram_range=(1, 1), analyzer=word, max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class numpy.int64>)[source]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "# count vectorizer params\n",
    "model_count = CountVectorizer\n",
    "\n",
    "params_count1 = ('word', r'\\w{1,}', 'english', True, (1,1))\n",
    "params_count2 = ('char', r'\\w{1,}', 'english', True, (1,1))\n",
    "params_count3 = ('word', r'\\w{1,}', 'english', True, (2,3))\n",
    "params_count4 = ('char', r'\\w{1,}', 'english', True, (2,3))\n",
    "\n",
    "xtrain_count,xvalid_count= run_vectorizer(train_xp,valid_xp, model_count, params_count1)\n",
    "xtrain_count_char,xvalid_count_char = run_vectorizer(train_xp,valid_xp, model_count, params_count2)\n",
    "xtrain_count_ngram,xvalid_count_ngram = run_vectorizer(train_xp,valid_xp, model_count, params_count3)\n",
    "xtrain_count_ngram_chars,xvalid_count_ngram_chars = run_vectorizer(train_xp,valid_xp, model_count, params_count4)\n",
    "\n",
    "# code you want to evaluate\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "time_keeper['count_vectorization'] = elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectors\n",
    "\n",
    "#### Tfidf vectorizer:\n",
    "- Strips out english\" stop words\n",
    "- Filters out terms that occur in more than half of the docs (max_df=0.5)\n",
    "- Filters out terms that occur in only one document (min_df=2).\n",
    "- Selects the 10,000 most frequently occuring words in the corpus.\n",
    "- Normalizes the vector (L2 norm of 1.0) to normalize the effect of document length on the tf-idf values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "# tfidf vectorizer params\n",
    "model_tfidf = TfidfVectorizer\n",
    "\n",
    "params_tfidf1 = ('word',0.5,2,10000,'english',True, r'\\w{1,}', (1,1))\n",
    "params_tfidf2 = ('char',0.5,2,10000,'english',True, r'\\w{1,}', (1,1))\n",
    "params_tfidf3 = ('word',0.5,2,10000,'english',True, r'\\w{1,}', (2,3))\n",
    "params_tfidf4 = ('char',0.5,2,10000,'english',True, r'\\w{1,}', (2,3))\n",
    "\n",
    "xtrain_tfidf,xvalid_tfidf= run_vectorizer(train_xp,valid_xp, model_tfidf, params_tfidf1)\n",
    "xtrain_tfidf_char,xvalid_tfidf_char = run_vectorizer(train_xp,valid_xp, model_tfidf, params_tfidf2)\n",
    "xtrain_tfidf_ngram,xvalid_tfidf_ngram = run_vectorizer(train_xp,valid_xp, model_tfidf, params_tfidf3)\n",
    "xtrain_tfidf_ngram_chars,xvalid_tfidf_ngram_chars = run_vectorizer(train_xp,valid_xp, model_tfidf, params_tfidf4)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "time_keeper['tfidf_vectorization'] = elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit run_vectorizer(train_xp,valid_xp, model_tfidf, params_tfidf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text / NLP based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NLP_features(texts, normed_=True):\n",
    "\n",
    "    trainDF=pd.DataFrame()\n",
    "    trainDF['char_count'] = texts.apply(len)\n",
    "    trainDF['word_count'] = texts.apply(lambda x: len(x.split()))\n",
    "    trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
    "    trainDF['punctuation_count'] = texts.apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "    trainDF['title_word_count'] = texts.apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "    trainDF['upper_case_word_count'] = texts.apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "    \n",
    "    pos_family = {\n",
    "        'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "        'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "        'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "        'adj' :  ['JJ','JJR','JJS'],\n",
    "        'adv' : ['RB','RBR','RBS','WRB']\n",
    "    }\n",
    "    \n",
    "    # function to check and get the part of speech tag count of a words in a given sentence\n",
    "    def check_pos_tag(x, flag):\n",
    "        cnt = 0\n",
    "        try:\n",
    "            wiki = textblob.TextBlob(x)\n",
    "            for tup in wiki.tags:\n",
    "                ppo = list(tup)[1]\n",
    "                if ppo in pos_family[flag]:\n",
    "                    cnt += 1\n",
    "        except:\n",
    "            pass\n",
    "        return cnt\n",
    "    \n",
    "    trainDF['noun_count'] = texts.apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "    trainDF['verb_count'] = texts.apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "    trainDF['adj_count'] = texts.apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "    trainDF['adv_count'] = texts.apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "    trainDF['pron_count'] = texts.apply(lambda x: check_pos_tag(x, 'pron'))\n",
    "    \n",
    "    if normed_:\n",
    "        \n",
    "        x = trainDF.values #returns a numpy array\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        x_scaled = min_max_scaler.fit_transform(x)\n",
    "        trainDF_normed = pd.DataFrame(x_scaled)\n",
    "        \n",
    "        return trainDF_normed\n",
    "    \n",
    "    return trainDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP_features = get_NLP_features(train_xp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Model\n",
    "\n",
    "-  https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_dictionary(dictionary, n=10):\n",
    "    '''\n",
    "    Returns: the first n words from dictionary \n",
    "    '''\n",
    "    \n",
    "    count = 0\n",
    "    for k, v in dictionary.iteritems():\n",
    "        print(k, v)\n",
    "        count += 1\n",
    "        if count > n:\n",
    "            break\n",
    "            \n",
    "# list_dictionary(dictionary, top=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_corpus(corpus,doc_to_insp=10):\n",
    "    doc_insp = corpus[doc_to_insp]\n",
    "    for i in range(len(doc_insp)):\n",
    "        print(\"Word {} (\\\"{}\\\") appears {} time.\".format(doc_insp[i][0],dictionary[doc_insp[i][0]],doc_insp[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print topics from lda_model\n",
    "def get_topics(model):\n",
    "    for idx, topic in model.print_topics(-1):\n",
    "        print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "# get_topics(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect where doc would be classified\n",
    "def lda_in_doc_classification(model, corpus, doc_num=100):\n",
    "    for index, score in sorted(model[corpus[doc_num]], key=lambda tup: -1*tup[1]):\n",
    "        print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, model.print_topic(index, 10)))\n",
    "# lda_in_doc_classification(lda_model, bow_corpus, 20)\n",
    "# lda_in_doc_classification(lda_model_tfidf, bow_corpus, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_out_doc_classification(unseen_document):\n",
    "    '''\n",
    "    Takes a list of a single doc string\n",
    "    example: unseen_document = ['The night love tomorrow']\n",
    "    '''\n",
    "    text = pre_processing_steps(pd.DataFrame({'texts': unseen_document})).processed_text[0]\n",
    "    bow_vector = dictionary.doc2bow(text)\n",
    "    for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "        print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary, bow corpus, and tfidf corpus (using bow corpus)\n",
    "dictionary = corpora.Dictionary(train_xp)\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.1, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in train_xp]\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running LDA using Bag of Words\n",
    "lda_model = models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running LDA using TF-IDF\n",
    "lda_model_tfidf = models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "# get_topics(lda_model_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_model_tfidf.show_topics(10)\n",
    "# lda_model_tfidf.get_topics().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA TFIDF similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "# https://www.oreilly.com/learning/how-do-i-compare-document-similarity-using-python\n",
    "TFIDF = models.TfidfModel(corpus)\n",
    "sims = similarities.Similarity('temp' , TFIDF[corpus], num_features = len(dictionary))\n",
    "q_doc = dictionary.doc2bow(text) # prediction similarity --> query doc\n",
    "q_doc_TFIDF = TFIDF[q_doc]\n",
    "temp_lst=[]\n",
    "for idx,i in enumerate(sims[q_doc_TFIDF]):\n",
    "    if i != 0.:\n",
    "        temp_lst.append((idx,i))\n",
    "        \n",
    "# inspect_corpus(corpus,1704)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, train_x, train_y, valid_x, valid_y, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    print('Running model...')\n",
    "    start_time = timeit.default_timer()\n",
    "    classifier.fit(train_x, train_y)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(valid_x)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print('Fetching report...') # builds a report from the training\n",
    "    report = classification_report(predictions,valid_y)\n",
    "    lines = report.split('\\n')\n",
    "    report_data = []\n",
    "    for line in lines[2:-3]:\n",
    "        row = {}\n",
    "        row_data = line.split('      ')\n",
    "        row['class'] = row_data[1]\n",
    "        row['precision'] = float(row_data[2])\n",
    "        row['recall'] = float(row_data[3])\n",
    "        row['f1_score'] = float(row_data[4])\n",
    "        row['support'] = int(row_data[5])\n",
    "        row['time_secs'] = elapsed\n",
    "        report_data.append(row)\n",
    "    dataframe = pd.DataFrame(report_data)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(valid_y, y_predicted):\n",
    "    cm = confusion_matrix(valid_y, y_predicted)\n",
    "    df_cm = pd.DataFrame(cm)\n",
    "    fig = plt.figure(figsize=(4,3))\n",
    "    heatmap = sns.heatmap(df_cm,annot=True,cmap='Blues', fmt='g', cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_out(report,df_title):\n",
    "    '''\n",
    "    Builds a dataframe from a classification report\n",
    "    '''\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    for line in lines[2:-3]:\n",
    "        row = {}\n",
    "        row_data = line.split('      ')\n",
    "        row['class'] = row_data[0]\n",
    "        row['precision'] = float(row_data[1])\n",
    "        row['recall'] = float(row_data[2])\n",
    "        row['f1_score'] = float(row_data[3])\n",
    "        row['support'] = float(row_data[4])\n",
    "        report_data.append(row)\n",
    "    dataframe = pd.DataFrame.from_dict(report_data)\n",
    "    timex = time.strftime(\"%a_%d_%b_%Y_%H:%M:%S\", time.gmtime())\n",
    "    dataframe.to_csv('classification_report_{}_{}.csv'.format(df_title,timex), index = False)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "#report = classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model...\n",
      "Fetching report...\n",
      "Running model...\n",
      "Fetching report...\n",
      "Running model...\n",
      "Fetching report...\n",
      "Running model...\n",
      "Fetching report...\n",
      "Running model...\n",
      "Fetching report...\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Naive Bayes on Count Vectors\n",
    "score1 = train_model(MultinomialNB(), xtrain_count, train_yp, xvalid_count, valid_yp)\n",
    "#print(\"NB, Count Vectors: \", score)\n",
    "\n",
    "score2 = train_model(MultinomialNB(), xtrain_count_char, train_yp, xvalid_count_char, valid_yp)\n",
    "#print(\"NB, Count Vectors Char: \", score)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "score3 = train_model(MultinomialNB(), xtrain_tfidf, train_yp, xvalid_tfidf, valid_yp)\n",
    "#print(\"NB, WordLevel TF-IDF: \", score)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "score4 = train_model(MultinomialNB(), xtrain_tfidf_ngram, train_yp, xvalid_tfidf_ngram, valid_yp)\n",
    "#print(\"NB, N-Gram Vectors: \", score)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "score5 = train_model(MultinomialNB(), xtrain_tfidf_ngram_chars, train_yp, xvalid_tfidf_ngram_chars, valid_yp)\n",
    "#print(\"NB, CharLevel Vectors: \", score)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "time_keeper['NB']=elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "      <th>time_secs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NB, Count Vectors:</th>\n",
       "      <td>0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1383</td>\n",
       "      <td>2.143626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB, Count Vectors:</th>\n",
       "      <td>1</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1680</td>\n",
       "      <td>2.143626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB, Count Vectors Char:</th>\n",
       "      <td>0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1393</td>\n",
       "      <td>2.832410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB, Count Vectors Char:</th>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1670</td>\n",
       "      <td>2.832410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB, WordLevel TF-IDF:</th>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1497</td>\n",
       "      <td>2.096950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB, WordLevel TF-IDF:</th>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1566</td>\n",
       "      <td>2.096950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB, N-Gram Vectors:</th>\n",
       "      <td>0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1485</td>\n",
       "      <td>1.352976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB, N-Gram Vectors:</th>\n",
       "      <td>1</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1578</td>\n",
       "      <td>1.352976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB, CharLevel Vectors:</th>\n",
       "      <td>0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1485</td>\n",
       "      <td>1.380291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB, CharLevel Vectors:</th>\n",
       "      <td>1</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1578</td>\n",
       "      <td>1.380291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          class  f1_score  precision  recall  support  \\\n",
       "                                                                        \n",
       "NB, Count Vectors:            0      0.96       0.92    1.00     1383   \n",
       "NB, Count Vectors:            1      0.96       1.00    0.93     1680   \n",
       "NB, Count Vectors Char:       0      0.94       0.91    0.98     1393   \n",
       "NB, Count Vectors Char:       1      0.95       0.98    0.92     1670   \n",
       "NB, WordLevel TF-IDF:         0      1.00       0.99    1.00     1497   \n",
       "NB, WordLevel TF-IDF:         1      1.00       1.00    0.99     1566   \n",
       "NB, N-Gram Vectors:           0      0.99       0.99    1.00     1485   \n",
       "NB, N-Gram Vectors:           1      0.99       1.00    0.99     1578   \n",
       "NB, CharLevel Vectors:        0      0.99       0.99    1.00     1485   \n",
       "NB, CharLevel Vectors:        1      0.99       1.00    0.99     1578   \n",
       "\n",
       "                          time_secs  \n",
       "                                     \n",
       "NB, Count Vectors:         2.143626  \n",
       "NB, Count Vectors:         2.143626  \n",
       "NB, Count Vectors Char:    2.832410  \n",
       "NB, Count Vectors Char:    2.832410  \n",
       "NB, WordLevel TF-IDF:      2.096950  \n",
       "NB, WordLevel TF-IDF:      2.096950  \n",
       "NB, N-Gram Vectors:        1.352976  \n",
       "NB, N-Gram Vectors:        1.352976  \n",
       "NB, CharLevel Vectors:     1.380291  \n",
       "NB, CharLevel Vectors:     1.380291  "
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save joined results\n",
    "names = [\"NB, Count Vectors:\",\"NB, Count Vectors:\",\"NB, Count Vectors Char: \",\"NB, Count Vectors Char: \",\"NB, WordLevel TF-IDF: \",\n",
    "\"NB, WordLevel TF-IDF: \",\"NB, N-Gram Vectors: \",\"NB, N-Gram Vectors: \",\"NB, CharLevel Vectors: \",\"NB, CharLevel Vectors: \"]\n",
    "df_nb = score1.append(score2).append(score3).append(score4).append(score5)\n",
    "df_nb[' ']=names\n",
    "df_nb = df_nb.set_index(' ')\n",
    "df_nb.to_csv('classification_report_climate_{}_nb.csv'.format(df_name), index = False)\n",
    "df_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model...\n",
      "Fetching report...\n",
      "Running model...\n",
      "Fetching report...\n",
      "Running model...\n",
      "Fetching report...\n",
      "Running model...\n",
      "Fetching report...\n",
      "Running model...\n",
      "Fetching report...\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "score1 = train_model(KNeighborsClassifier(), xtrain_count, train_yp, xvalid_count, valid_yp)\n",
    "# print(\"KNN, Count Vectors: \", score)\n",
    "\n",
    "score2 = train_model(KNeighborsClassifier(), xtrain_count_char, train_yp, xvalid_count_char, valid_yp)\n",
    "# print(\"KNN, Count Vectors Char: \", score)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "score3 = train_model(KNeighborsClassifier(), xtrain_tfidf, train_yp, xvalid_tfidf, valid_yp)\n",
    "# print(\"KNN, WordLevel TF-IDF: \", score)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "score4 = train_model(KNeighborsClassifier(), xtrain_tfidf_ngram, train_yp, xvalid_tfidf_ngram, valid_yp)\n",
    "# print(\"KNN, N-Gram Vectors: \", score)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "score5 = train_model(KNeighborsClassifier(), xtrain_tfidf_ngram_chars, train_yp, xvalid_tfidf_ngram_chars, valid_yp)\n",
    "#print(\"KNN, CharLevel Vectors: \", score)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "time_keeper['KNN']=elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "      <th>time_secs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model_Vec</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNN, Count Vectors:</th>\n",
       "      <td>0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1383</td>\n",
       "      <td>2.170615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN, Count Vectors:</th>\n",
       "      <td>1</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1680</td>\n",
       "      <td>2.170615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN, Count Vectors Char:</th>\n",
       "      <td>0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1393</td>\n",
       "      <td>2.708080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN, Count Vectors Char:</th>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1670</td>\n",
       "      <td>2.708080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN, WordLevel TF-IDF:</th>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1497</td>\n",
       "      <td>2.039410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN, WordLevel TF-IDF:</th>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1566</td>\n",
       "      <td>2.039410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN, N-Gram Vectors:</th>\n",
       "      <td>0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1485</td>\n",
       "      <td>1.353920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN, N-Gram Vectors:</th>\n",
       "      <td>1</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1578</td>\n",
       "      <td>1.353920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN, CharLevel Vectors:</th>\n",
       "      <td>0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1485</td>\n",
       "      <td>1.336275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN, CharLevel Vectors:</th>\n",
       "      <td>1</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1578</td>\n",
       "      <td>1.336275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           class  f1_score  precision  recall  support  \\\n",
       "Model_Vec                                                                \n",
       "KNN, Count Vectors:            0      0.96       0.92    1.00     1383   \n",
       "KNN, Count Vectors:            1      0.96       1.00    0.93     1680   \n",
       "KNN, Count Vectors Char:       0      0.94       0.91    0.98     1393   \n",
       "KNN, Count Vectors Char:       1      0.95       0.98    0.92     1670   \n",
       "KNN, WordLevel TF-IDF:         0      1.00       0.99    1.00     1497   \n",
       "KNN, WordLevel TF-IDF:         1      1.00       1.00    0.99     1566   \n",
       "KNN, N-Gram Vectors:           0      0.99       0.99    1.00     1485   \n",
       "KNN, N-Gram Vectors:           1      0.99       1.00    0.99     1578   \n",
       "KNN, CharLevel Vectors:        0      0.99       0.99    1.00     1485   \n",
       "KNN, CharLevel Vectors:        1      0.99       1.00    0.99     1578   \n",
       "\n",
       "                           time_secs  \n",
       "Model_Vec                             \n",
       "KNN, Count Vectors:         2.170615  \n",
       "KNN, Count Vectors:         2.170615  \n",
       "KNN, Count Vectors Char:    2.708080  \n",
       "KNN, Count Vectors Char:    2.708080  \n",
       "KNN, WordLevel TF-IDF:      2.039410  \n",
       "KNN, WordLevel TF-IDF:      2.039410  \n",
       "KNN, N-Gram Vectors:        1.353920  \n",
       "KNN, N-Gram Vectors:        1.353920  \n",
       "KNN, CharLevel Vectors:     1.336275  \n",
       "KNN, CharLevel Vectors:     1.336275  "
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save joined results\n",
    "names = [\"KNN, Count Vectors:\",\"KNN, Count Vectors:\",\"KNN, Count Vectors Char: \",\"KNN, Count Vectors Char: \",\"KNN, WordLevel TF-IDF: \",\n",
    "\"KNN, WordLevel TF-IDF: \",\"KNN, N-Gram Vectors: \",\"KNN, N-Gram Vectors: \",\"KNN, CharLevel Vectors: \",\"KNN, CharLevel Vectors: \"]\n",
    "df_knn = score1.append(score2).append(score3).append(score4).append(score5)\n",
    "df_knn['Model_Vec']=names\n",
    "df_knn = df_knn.set_index('Model_Vec')\n",
    "df_knn.to_csv('classification_report_climate_{}_knn.csv'.format(df_name))\n",
    "df_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model...\n",
      "Fetching report...\n",
      "Running model...\n",
      "Fetching report...\n",
      "Running model...\n",
      "Fetching report...\n",
      "Running model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silas/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching report...\n",
      "Running model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silas/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching report...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silas/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# SVM N-Gram Chars on TF IDF Vectors\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "score1 = train_model(svm.SVC(), xtrain_count, train_yp, xvalid_count, valid_yp)\n",
    "# print(\"SVM, Count Vectors: \", score)\n",
    "\n",
    "score2 = train_model(svm.SVC(), xtrain_count_char, train_yp, xvalid_count_char, valid_yp)\n",
    "# print(\"SVM, Count Vectors Char: \", score)\n",
    "\n",
    "# SVM on Word Level TF IDF Vectors\n",
    "score3 = train_model(svm.SVC(), xtrain_tfidf, train_yp, xvalid_tfidf, valid_yp)\n",
    "# print(\"SVM, WordLevel TF-IDF: \", score)\n",
    "\n",
    "# SVM on Ngram Level TF IDF Vectors\n",
    "score4 = train_model(svm.SVC(), xtrain_tfidf_ngram, train_yp, xvalid_tfidf_ngram, valid_yp)\n",
    "# print(\"SVM, N-Gram Vectors: \", score)\n",
    "\n",
    "# SVM on Character Level TF IDF Vectors\n",
    "score5 = train_model(svm.SVC(), xtrain_tfidf_ngram_chars, train_yp, xvalid_tfidf_ngram_chars, valid_yp)\n",
    "#print(\"SVM, CharLevel Vectors: \", score)\n",
    "\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "time_keeper['SVM']=(elapsed, df_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "      <th>time_secs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model_Vec</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVM, Count Vectors:</th>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1502</td>\n",
       "      <td>0.841664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM, Count Vectors:</th>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1561</td>\n",
       "      <td>0.841664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM, Count Vectors Char:</th>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1502</td>\n",
       "      <td>0.384659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM, Count Vectors Char:</th>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1561</td>\n",
       "      <td>0.384659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM, WordLevel TF-IDF:</th>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1512</td>\n",
       "      <td>1.103367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM, WordLevel TF-IDF:</th>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1551</td>\n",
       "      <td>1.103367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM, N-Gram Vectors:</th>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1512</td>\n",
       "      <td>1.086589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM, N-Gram Vectors:</th>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1551</td>\n",
       "      <td>1.086589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM, CharLevel Vectors:</th>\n",
       "      <td>0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>3063</td>\n",
       "      <td>102.593964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM, CharLevel Vectors:</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>102.593964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           class  f1_score  precision  recall  support  \\\n",
       "Model_Vec                                                                \n",
       "SVM, Count Vectors:            0      1.00       1.00    1.00     1502   \n",
       "SVM, Count Vectors:            1      1.00       1.00    1.00     1561   \n",
       "SVM, Count Vectors Char:       0      1.00       1.00    1.00     1502   \n",
       "SVM, Count Vectors Char:       1      1.00       1.00    1.00     1561   \n",
       "SVM, WordLevel TF-IDF:         0      1.00       1.00    0.99     1512   \n",
       "SVM, WordLevel TF-IDF:         1      1.00       0.99    1.00     1551   \n",
       "SVM, N-Gram Vectors:           0      1.00       1.00    0.99     1512   \n",
       "SVM, N-Gram Vectors:           1      1.00       0.99    1.00     1551   \n",
       "SVM, CharLevel Vectors:        0      0.66       1.00    0.49     3063   \n",
       "SVM, CharLevel Vectors:        1      0.00       0.00    0.00        0   \n",
       "\n",
       "                            time_secs  \n",
       "Model_Vec                              \n",
       "SVM, Count Vectors:          0.841664  \n",
       "SVM, Count Vectors:          0.841664  \n",
       "SVM, Count Vectors Char:     0.384659  \n",
       "SVM, Count Vectors Char:     0.384659  \n",
       "SVM, WordLevel TF-IDF:       1.103367  \n",
       "SVM, WordLevel TF-IDF:       1.103367  \n",
       "SVM, N-Gram Vectors:         1.086589  \n",
       "SVM, N-Gram Vectors:         1.086589  \n",
       "SVM, CharLevel Vectors:    102.593964  \n",
       "SVM, CharLevel Vectors:    102.593964  "
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save joined results\n",
    "names = [\"SVM, Count Vectors:\",\"SVM, Count Vectors:\",\"SVM, Count Vectors Char: \",\"SVM, Count Vectors Char: \",\"SVM, WordLevel TF-IDF: \",\n",
    "\"SVM, WordLevel TF-IDF: \",\"SVM, N-Gram Vectors: \",\"SVM, N-Gram Vectors: \",\"SVM, CharLevel Vectors: \",\"SVM, CharLevel Vectors: \"]\n",
    "df_svm = score1.append(score2).append(score3).append(score4).append(score5)\n",
    "df_svm['Model_Vec']=names\n",
    "df_svm = df_svm.set_index('Model_Vec')\n",
    "df_svm.to_csv('classification_report_climate_{}_svm.csv'.format(df_name))\n",
    "df_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silas/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                            alpha=1e-3, n_iter=5, random_state=42)),])\n",
    "_ = text_clf_svm.fit(train_xp, train_yp)\n",
    "predicted_svm = text_clf_svm.predict(valid_xp)\n",
    "np.mean(predicted_svm == valid_yp)\n",
    "\n",
    "\n",
    "df_svm_exp1 = classification_report_out(report,\"climate_reuters\")\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "time_keeper['SVM_exp.1']=(elapsed, df_svm_exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>actual</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4915</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>petroleum exploration ltd year loss shr loss d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>metropolitan federal acquisition metropolitan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8795</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>oneok qtr feb net shr dlrs dlrs net mln mln re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7217</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>japan intend cut discount rate sumita bank jap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>force jurisdiction state regional date policy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>policy type policy support research developmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3421</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>regulatory instruments policy target multiple ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>force jurisdiction national date policy type r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>jurisdiction national date date policy type ec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3837</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>elektriciteitsproductie country netherlands po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pred  actual                                               text\n",
       "4915     0       0  petroleum exploration ltd year loss shr loss d...\n",
       "2200     0       0  metropolitan federal acquisition metropolitan ...\n",
       "8795     0       0  oneok qtr feb net shr dlrs dlrs net mln mln re...\n",
       "7217     0       0  japan intend cut discount rate sumita bank jap...\n",
       "420      1       1  force jurisdiction state regional date policy ...\n",
       "1511     1       1  policy type policy support research developmen...\n",
       "3421     1       1  regulatory instruments policy target multiple ...\n",
       "978      1       1  force jurisdiction national date policy type r...\n",
       "2217     1       1  jurisdiction national date date policy type ec...\n",
       "3837     1       1  elektriciteitsproductie country netherlands po..."
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'pred':predicted_svm, 'actual':valid_yp, 'text': valid_xp}).sample(10).sort_values(by='pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM grid search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   40.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 params - {'vect__ngram_range': (1, 1)}; mean - 1.00; std - 0.00\n",
      "1 params - {'vect__ngram_range': (1, 2)}; mean - 1.00; std - 0.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "#\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n",
    "        ('clf', LinearSVC(C=1000)),\n",
    "    ])\n",
    "\n",
    "# TASK: Build a grid search to find out whether unigrams or bigrams are\n",
    "# more useful.\n",
    "# Fit the pipeline on the training set using grid search for the parameters\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "}\n",
    "kfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, cv=kfolds,verbose=1)\n",
    "grid_search.fit(train_xp, train_yp)\n",
    "\n",
    "# TASK: print the mean and std for each candidate along with the parameter\n",
    "# settings for all the candidates explored by grid search.\n",
    "n_candidates = len(grid_search.cv_results_['params'])\n",
    "for i in range(n_candidates):\n",
    "    print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
    "             % (grid_search.cv_results_['params'][i],\n",
    "                grid_search.cv_results_['mean_test_score'][i],\n",
    "                grid_search.cv_results_['std_test_score'][i]))\n",
    "\n",
    "y_predicted = grid_search.predict(valid_xp)\n",
    "\n",
    "# Print the classification report\n",
    "report = classification_report(valid_yp, y_predicted)\n",
    "\n",
    "# Print and plot the confusion matrix\n",
    "plot_confusion_matrix(valid_yp, y_predicted)\n",
    "\n",
    "df_svc = classification_report_out(report,\"climate_reuters\")\n",
    "\n",
    "# df_svc['Model_Vec']=names\n",
    "# df_svc = df_svm.set_index('Model_Vec')\n",
    "# df_svc.to_csv('classification_report_climate_reuters_svc.csv')\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "time_keeper['SVC_exp.2.grid']=(elapsed,df_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed: 10.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      1531\n",
      "          1       1.00      1.00      1.00      1532\n",
      "\n",
      "avg / total       1.00      1.00      1.00      3063\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQIAAADFCAYAAAC/1fzoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAACY9JREFUeJzt2m1sVfUdwPHvX0uRjRFQ1luQUqdFFsAHAmiCypQXDlGDPEUUYxQUNcG5GI0wpjixiMyZRTGp4HAEMmBOsuFg+AJBEEVh6HgSNhCBDloeioIIA+797wVY5bG1pT0tfD8JyT0P957f5cI355x7Q4wRSWe3c5IeQFLyDIEkQyDJEEjCEEjCEEjCEEjCEEjCEEgCsmr6AI06DvWni/XUriXjkh5B1XBeFqGy+3pGIMkQSDIEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkjAEkoCspAeoa4pGDuSmbh3YXraHzv1HAzDigZ4M6tOV7bu+AmDkuJm8/d5qOrfPZ9yTdwAQAhQWzWbmvOUnfR3VHYsWLuD5MYVk0hl69+3P4PuHJD1SogzBMSa/tZii6e/y2qi7j1r/8pR5/H7y3KPWrVq/hWsGjiWdzpDbvAkfTh/OrAUrSaczJ30dJS+dTjO68BlenfA6qVSKO2/vx/U3dOeSgoKkR0uMlwbHWLRsPWVffl2pffftP0g6nQGgYXYDYoxVeh3VrpUrlpOXl0+rvDwaZGfTo+fNzJ83t+InnsEqPCMIIfwU6AVcCERgCzAzxvhpDc9Wpzw4oBt33nIVy1ZvYtiLM/hizz4AunTIp+jpu2jd4nwG/3pSeRhUd20rLSW3RW75ck4qxYrlyxOcKHmnPCMIITwBTAMC8BGw5MjjqSGEYad43pAQwtIQwtJDO1adznkTMeGNhbS79WmuHjCGkh27GfNon/JtS1ZupFO/Qq69ayyPD7qRhtlebdV1kXjcuhBCApPUHRVdGgwGusQYx8QYpxz5Mwa46si2E4oxjo8xdo4xds5q3v50zpuIbWV7yGQiMUYmzlhE5w75x+2zdkMpe/cdoH1BywQm1PeRSuVSsrWkfHlbaSk5OTkJTpS8ikKQAU70L7vFkW1nhdzmTcof9+p+BavXbwUgv+UFnHvu4b/C1i2acelFKTZu2ZnIjKq89h0uY9Omzyku3szBAweYM3sWP7uhe9JjJaqi89hfAnNDCP8BNh9Z1xooAIbW5GBJmfTcPVzXqQ3NmzZm3ZxRjCqaTbdObbi8bStijGzcWsbDz04FoGvHi3ns3hs5eChNJhN5ZPR0dn6x96SvM+mvHyT51nREVlYWw0c8xUND7iOTSXNb774UFLRJeqxEhe/e6T7hDiGcw+FLgQs5fH+gGFgSY0xX5gCNOg499QFUZ+1aMi7pEVQN52VR6RsfFd7ZijFmgMXVmkhSnebvCCQZAkmGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBKGQBIQYow1eoD9h6jZA6jGNOsyNOkRVA37Ph4XKruvZwSSDIEkQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJQyAJyEp6gPps0cIFPD+mkEw6Q+++/Rl8/5CkRzrrFY0cyE3dOrC9bA+d+48GYMQDPRnUpyvbd30FwMhxM3n7vdV0bp/PuCfvACAEKCyazcx5y2mVaspro+4mdUETMjEy8c1FvDJ1flJvqVYYgipKp9OMLnyGVye8TiqV4s7b+3H9Dd25pKAg6dHOapPfWkzR9Hd5bdTdR61/eco8fj957lHrVq3fwjUDx5JOZ8ht3oQPpw9n1oKVHEpnGPbiDD5ZU0zjHzTk/T89wdwP17Dms5LafCu1ykuDKlq5Yjl5efm0ysujQXY2PXrezPx5cyt+omrUomXrKfvy60rtu2//QdLpDAANsxsQYwSgZMduPllTDMBXX/+PNRtKaPnjpjUzcB1R5RCEEO49nYPUN9tKS8ltkVu+nJNKUVpamuBEOpUHB3Tjo+nDKRo5kKY/alS+vkuHfP75lxEsfeNX/KJwWnkYvtG6xflc2bYVS1Z+XssT167qnBH85mQbQghDQghLQwhL/zBhfDUOUXdF4nHrQggJTKKKTHhjIe1ufZqrB4yhZMduxjzap3zbkpUb6dSvkGvvGsvjg26kYfa3V8s/bJTN1Bfu4/EX3mTP3v1JjF5rTnmPIISw/GSbgNTJnhdjHA+MB9h/6AT/Y84AqVQuJVu/vWbcVlpKTk5OghPpZLaV7Sl/PHHGIma89OBx+6zdUMrefQdoX9CSZas3kZV1DlNfuJ/p/1jK3975V22Om4iKbhamgJ8Du45ZH4D3a2SieqJ9h8vYtOlzios3k8pJMWf2LJ777e+SHksnkNu8CSU7dgPQq/sVrF6/FYD8lhdQXLqLdDpD6xbNuPSiFBu37AQOf/uwdkMJL015J7G5a1NFIfg70DjG+MmxG0II82tkonoiKyuL4SOe4qEh95HJpLmtd18KCtokPdZZb9Jz93BdpzY0b9qYdXNGMapoNt06teHytq2IMbJxaxkPPzsVgK4dL+axe2/k4KE0mUzkkdHT2fnFXrpeeTEDb7maFf/+L4unDQO+/crxTBW+uVNaU87US4OzQbMuQ5MeQdWw7+Nxlb5p5deHkgyBJEMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCUMgCQgxxqRnqNdCCENijOOTnkNV4+d3mGcE1Tck6QFULX5+GAJJGAJJGILT4ay/vqzn/PzwZqEkPCOQhCGQhCGolhBCjxDC2hDCuhDCsKTnUeWFECaGELaFEFYmPUtdYAiqKIRwLvAKcBPQDrgjhNAu2an0PfwR6JH0EHWFIai6q4B1McbPYowHgGlAr4RnUiXFGBcAZUnPUVcYgqq7ENj8neXiI+ukescQVF04wTq/i1W9ZAiqrhjI+85yK2BLQrNI1WIIqm4J0CaE8JMQQjYwAJiZ8ExSlRiCKooxHgKGAm8DnwJ/jjGuSnYqVVYIYSrwAdA2hFAcQhic9ExJ8ifGkjwjkGQIJGEIJGEIJGEIJGEIJGEIJAH/B0/2j9Q5q4P+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# en_stopwords = set(stopwords.words(\"english\")) \n",
    "from text_processing import tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenizer,\n",
    "    lowercase = True,\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words = stopwords.words('english'))\n",
    "\n",
    "kfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "pipeline_svm = make_pipeline(vectorizer, \n",
    "                            SVC(probability=True, kernel=\"linear\", class_weight=\"balanced\"))\n",
    "\n",
    "\n",
    "grid_svm = GridSearchCV(pipeline_svm,\n",
    "                    param_grid = {'svc__C': [0.01, 0.1, 1]}, \n",
    "                    cv = kfolds,\n",
    "                    scoring=\"roc_auc\",\n",
    "                    verbose=1,   \n",
    "                    n_jobs=-1) \n",
    "\n",
    "grid_svm.fit(train_x, train_y)\n",
    "grid_svm.score(valid_x, valid_y)\n",
    "\n",
    "\n",
    "predictions_svm_ = grid_svm.predict(valid_x)\n",
    "cm = confusion_matrix(valid_y, predictions_svm_)\n",
    "print(classification_report(valid_y,predictions_svm_))\n",
    "plot_confusion_matrix(valid_y, predictions_svm_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect errors\n",
    "# error_df = pd.DataFrame({'val_text':valid_x, 'val_true':valid_y, 'val_pred':predictions_svm_})\n",
    "# error_df[(error_df['val_pred']!=error_df['val_true'])].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model...\n",
      "Fetching report...\n",
      "Running model...\n",
      "Fetching report...\n",
      "Running model...\n",
      "Fetching report...\n",
      "Running model...\n",
      "Fetching report...\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Logistic Regression on Count Vectors\n",
    "score1 = train_model(LogisticRegression(), xtrain_count, train_yp, xvalid_count, valid_yp)\n",
    "#print(\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# Logistic Regression on Word Level TF IDF Vectors\n",
    "score2 = train_model(LogisticRegression(), xtrain_tfidf, train_yp, xvalid_tfidf, valid_yp)\n",
    "#print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Logistic Regression on Ngram Level TF IDF Vectors\n",
    "score3 = train_model(LogisticRegression(), xtrain_tfidf_ngram, train_yp, xvalid_tfidf_ngram, valid_yp)\n",
    "#print(\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Logistic Regression on Character Level TF IDF Vectors\n",
    "score4 = train_model(LogisticRegression(), xtrain_tfidf_ngram_chars, train_yp, xvalid_tfidf_ngram_chars, valid_yp)\n",
    "#print(\"LR, CharLevel Vectors: \", accuracy)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "time_keeper['Logistic_Regression']=elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "      <th>time_secs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model_Vec</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LRM, Count Vectors:</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1502</td>\n",
       "      <td>0.841664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LRM, Count Vectors:</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1561</td>\n",
       "      <td>0.841664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LRM, WordLevel TF-IDF:</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1502</td>\n",
       "      <td>0.384659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LRM, WordLevel TF-IDF:</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1561</td>\n",
       "      <td>0.384659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LRM, N-Gram Vectors:</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1512</td>\n",
       "      <td>1.103367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LRM, N-Gram Vectors:</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1551</td>\n",
       "      <td>1.103367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LRM, CharLevel Vectors:</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1512</td>\n",
       "      <td>1.086589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LRM, CharLevel Vectors:</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1551</td>\n",
       "      <td>1.086589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          class  f1_score  precision  recall  support  \\\n",
       "Model_Vec                                                               \n",
       "LRM, Count Vectors:           0       1.0       1.00    1.00     1502   \n",
       "LRM, Count Vectors:           1       1.0       1.00    1.00     1561   \n",
       "LRM, WordLevel TF-IDF:        0       1.0       1.00    1.00     1502   \n",
       "LRM, WordLevel TF-IDF:        1       1.0       1.00    1.00     1561   \n",
       "LRM, N-Gram Vectors:          0       1.0       1.00    0.99     1512   \n",
       "LRM, N-Gram Vectors:          1       1.0       0.99    1.00     1551   \n",
       "LRM, CharLevel Vectors:       0       1.0       1.00    0.99     1512   \n",
       "LRM, CharLevel Vectors:       1       1.0       0.99    1.00     1551   \n",
       "\n",
       "                          time_secs  \n",
       "Model_Vec                            \n",
       "LRM, Count Vectors:        0.841664  \n",
       "LRM, Count Vectors:        0.841664  \n",
       "LRM, WordLevel TF-IDF:     0.384659  \n",
       "LRM, WordLevel TF-IDF:     0.384659  \n",
       "LRM, N-Gram Vectors:       1.103367  \n",
       "LRM, N-Gram Vectors:       1.103367  \n",
       "LRM, CharLevel Vectors:    1.086589  \n",
       "LRM, CharLevel Vectors:    1.086589  "
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save joined results\n",
    "names = [\"LRM, Count Vectors:\",\"LRM, Count Vectors:\",\"LRM, WordLevel TF-IDF: \",\n",
    "\"LRM, WordLevel TF-IDF: \",\"LRM, N-Gram Vectors: \",\"LRM, N-Gram Vectors: \",\"LRM, CharLevel Vectors: \",\"LRM, CharLevel Vectors: \"]\n",
    "df_LRM = score1.append(score2).append(score3).append(score4)\n",
    "df_LRM['Model_Vec']=names\n",
    "df_LRM = df_LRM.set_index('Model_Vec')\n",
    "df_LRM.to_csv('classification_report_climate_{}_LRM.csv'.format(df_name))\n",
    "df_LRM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model...\n",
      "Fetching report...\n",
      "Running model...\n",
      "Fetching report...\n",
      "Running model...\n",
      "Fetching report...\n",
      "Running model...\n",
      "Fetching report...\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Random Forest Classifier on Count Vectors\n",
    "score1 = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_yp, xvalid_count, valid_yp)\n",
    "#print(\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# Random Forest Classifier on Word Level TF IDF Vectors\n",
    "score2 = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_yp, xvalid_tfidf, valid_yp)\n",
    "#print(\"RF, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Random Forest Classifier on Ngram Level TF IDF Vectors\n",
    "score3 = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram, train_yp, xvalid_tfidf_ngram, valid_yp)\n",
    "#print(\"RF, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Random Forest Classifier on Character Level TF IDF Vectors\n",
    "score4 = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram_chars, train_yp, xvalid_tfidf_ngram_chars, valid_yp)\n",
    "#print(\"RF, CharLevel Vectors: \", accuracy)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "time_keeper['Random_Forest']=(elapsed,df_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "      <th>time_secs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model_Vec</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RF, Count Vectors:</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1504</td>\n",
       "      <td>0.786027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF, Count Vectors:</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1559</td>\n",
       "      <td>0.786027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF, WordLevel TF-IDF:</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1503</td>\n",
       "      <td>0.625605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF, WordLevel TF-IDF:</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1560</td>\n",
       "      <td>0.625605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF, N-Gram Vectors:</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1503</td>\n",
       "      <td>14.580575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF, N-Gram Vectors:</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1560</td>\n",
       "      <td>14.580575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF, CharLevel Vectors:</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1503</td>\n",
       "      <td>13.900107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF, CharLevel Vectors:</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1560</td>\n",
       "      <td>13.900107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         class  f1_score  precision  recall  support  \\\n",
       "Model_Vec                                                              \n",
       "RF, Count Vectors:           0       1.0        1.0     1.0     1504   \n",
       "RF, Count Vectors:           1       1.0        1.0     1.0     1559   \n",
       "RF, WordLevel TF-IDF:        0       1.0        1.0     1.0     1503   \n",
       "RF, WordLevel TF-IDF:        1       1.0        1.0     1.0     1560   \n",
       "RF, N-Gram Vectors:          0       1.0        1.0     1.0     1503   \n",
       "RF, N-Gram Vectors:          1       1.0        1.0     1.0     1560   \n",
       "RF, CharLevel Vectors:       0       1.0        1.0     1.0     1503   \n",
       "RF, CharLevel Vectors:       1       1.0        1.0     1.0     1560   \n",
       "\n",
       "                         time_secs  \n",
       "Model_Vec                           \n",
       "RF, Count Vectors:        0.786027  \n",
       "RF, Count Vectors:        0.786027  \n",
       "RF, WordLevel TF-IDF:     0.625605  \n",
       "RF, WordLevel TF-IDF:     0.625605  \n",
       "RF, N-Gram Vectors:      14.580575  \n",
       "RF, N-Gram Vectors:      14.580575  \n",
       "RF, CharLevel Vectors:   13.900107  \n",
       "RF, CharLevel Vectors:   13.900107  "
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save joined results\n",
    "names = [\"RF, Count Vectors:\",\"RF, Count Vectors:\",\"RF, WordLevel TF-IDF: \",\n",
    "\"RF, WordLevel TF-IDF: \",\"RF, N-Gram Vectors: \",\"RF, N-Gram Vectors: \",\"RF, CharLevel Vectors: \",\"RF, CharLevel Vectors: \"]\n",
    "df_RF = score1.append(score2).append(score3).append(score4)\n",
    "df_RF['Model_Vec']=names\n",
    "df_RF = df_RF.set_index('Model_Vec')\n",
    "df_RF.to_csv('classification_report_climate_{}_RF.csv'.format(df_name))\n",
    "df_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(time_keeper).to_csv('{}_merged_run_times.csv'.format(df_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('./Data/wiki-news-300d-1M.vec')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer in keras\n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(texts)\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    return classifier \n",
    "\n",
    "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
    "accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
    "print(\"NN, Ngram Level TF IDF Vectors\",  accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_cnn()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"CNN, Word Embeddings\",  accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_lstm():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_lstm()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"RNN-LSTM, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirectional_rnn():\n",
    "# Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_bidirectional_rnn()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"RNN-Bidirectional, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rcnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "    \n",
    "    # Add the recurrent layer\n",
    "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
    "    \n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rcnn()\n",
    "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "print(\"CNN, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a graph session\n",
    "sess = tf.Session()\n",
    "\n",
    "batch_size = 200\n",
    "max_features = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up data set into train/test\n",
    "train_indices = np.random.choice(sparse_tfidf_texts.shape[0], round(0.8*sparse_tfidf_texts.shape[0]), replace=False)\n",
    "test_indices = np.array(list(set(range(sparse_tfidf_texts.shape[0])) - set(train_indices)))\n",
    "texts_train = sparse_tfidf_texts[train_indices]\n",
    "texts_test = sparse_tfidf_texts[test_indices]\n",
    "target_train = np.array([x for ix, x in enumerate(labels) if ix in train_indices])\n",
    "target_test = np.array([x for ix, x in enumerate(labels) if ix in test_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables for logistic regression\n",
    "A = tf.Variable(tf.random_normal(shape=[max_features,1]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "\n",
    "# Initialize placeholders\n",
    "x_data = tf.placeholder(shape=[None, max_features], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "# Declare logistic model (sigmoid in loss function)\n",
    "model_output = tf.add(tf.matmul(x_data, A), b)\n",
    "\n",
    "# Declare loss function (Cross Entropy loss)\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=y_target))\n",
    "\n",
    "# Actual Prediction\n",
    "prediction = tf.round(tf.sigmoid(model_output))\n",
    "predictions_correct = tf.cast(tf.equal(prediction, y_target), tf.float32)\n",
    "accuracy = tf.reduce_mean(predictions_correct)\n",
    "\n",
    "# Declare optimizer\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.0025)\n",
    "train_step = my_opt.minimize(loss)\n",
    "\n",
    "# Intitialize Variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Start Logistic Regression\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "i_data = []\n",
    "for i in range(10000):\n",
    "    rand_index = np.random.choice(texts_train.shape[0], size=batch_size)\n",
    "    rand_x = texts_train[rand_index].todense()\n",
    "    rand_y = np.transpose([target_train[rand_index]])\n",
    "    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    \n",
    "    # Only record loss and accuracy every 100 generations\n",
    "    if (i+1)%100==0:\n",
    "        i_data.append(i+1)\n",
    "        train_loss_temp = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "        train_loss.append(train_loss_temp)\n",
    "        \n",
    "        test_loss_temp = sess.run(loss, feed_dict={x_data: texts_test.todense(), y_target: np.transpose([target_test])})\n",
    "        test_loss.append(test_loss_temp)\n",
    "        \n",
    "        train_acc_temp = sess.run(accuracy, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "        train_acc.append(train_acc_temp)\n",
    "    \n",
    "        test_acc_temp = sess.run(accuracy, feed_dict={x_data: texts_test.todense(), y_target: np.transpose([target_test])})\n",
    "        test_acc.append(test_acc_temp)\n",
    "    if (i+1)%500==0:\n",
    "        acc_and_loss = [i+1, train_loss_temp, test_loss_temp, train_acc_temp, test_acc_temp]\n",
    "        acc_and_loss = [np.round(x,2) for x in acc_and_loss]\n",
    "        print('Generation # {}. Train Loss (Test Loss): {:.2f} ({:.2f}). Train Acc (Test Acc): {:.2f} ({:.2f})'.format(*acc_and_loss))\n",
    "\n",
    "\n",
    "# Plot loss over time\n",
    "plt.plot(i_data, train_loss, 'k-', label='Train Loss')\n",
    "plt.plot(i_data, test_loss, 'r--', label='Test Loss', linewidth=4)\n",
    "plt.title('Cross Entropy Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plot train and test accuracy\n",
    "plt.plot(i_data, train_acc, 'k-', label='Train Set Accuracy')\n",
    "plt.plot(i_data, test_acc, 'r--', label='Test Set Accuracy', linewidth=4)\n",
    "plt.title('Train and Test Accuracy')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPARISON\n",
    "- https://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "# load dataset\n",
    "'''\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = train_y\n",
    "'''\n",
    "X = NLP_DF\n",
    "Y = train_y\n",
    "# prepare configuration for cross validation test harness\n",
    "seed = 7\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\tcv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
